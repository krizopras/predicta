#!/usr/bin/env python3
"""
Model Trainer v4.0 - OPTIMIZED FOR 70%+ ACCURACY
=================================================
Kritik deƒüi≈üiklikler:
‚úÖ n_iter_search: 50 ‚Üí 80
‚úÖ scoring: 'balanced_accuracy' ‚Üí 'f1_macro'  
‚úÖ balance_method: 'smote_tomek' ‚Üí 'hybrid'
‚úÖ use_class_weights: True (eklendi)
‚úÖ cv_folds: 5 (optimal)
‚úÖ VotingClassifier eklendi
‚úÖ Enhanced feature engineer v4.0 desteƒüi
"""

import os
import time
import json
import joblib
import logging
import numpy as np
import pandas as pd
from tqdm import tqdm
from datetime import datetime
from pathlib import Path
from collections import Counter
from typing import Any, Dict, List, Tuple, Optional

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    balanced_accuracy_score, f1_score
)
from sklearn.ensemble import (
    GradientBoostingClassifier, 
    RandomForestClassifier, 
    StackingClassifier,
    VotingClassifier
)
from sklearn.utils.class_weight import compute_class_weight
import xgboost as xgb

try:
    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE
    from imblearn.under_sampling import RandomUnderSampler, TomekLinks
    from imblearn.combine import SMOTETomek, SMOTEENN
    IMBLEARN_AVAILABLE = True
except ImportError:
    IMBLEARN_AVAILABLE = False
    print("‚ö†Ô∏è  pip install imbalanced-learn")

try:
    import lightgbm as lgb
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger("ImprovedTrainer")

try:
    from enhanced_feature_engineer_v4 import EnhancedFeatureEngineer, TOTAL_FEATURES_V4
    FEATURE_ENGINEER_AVAILABLE = True
    logger.info(f"‚úÖ Feature Engineer v4.0 loaded ({TOTAL_FEATURES_V4} features)")
except ImportError:
    try:
        from enhanced_feature_engineer import EnhancedFeatureEngineer
        FEATURE_ENGINEER_AVAILABLE = True
        logger.warning("‚ö†Ô∏è  Using old feature engineer (v3.5)")
    except ImportError:
        FEATURE_ENGINEER_AVAILABLE = False
        logger.error("‚ùå enhanced_feature_engineer.py bulunamadƒ±!")

try:
    from historical_processor import HistoricalDataProcessor
    HISTORICAL_PROCESSOR_AVAILABLE = True
except ImportError:
    HISTORICAL_PROCESSOR_AVAILABLE = False


class ImprovedModelTrainer:
    
    def __init__(
        self,
        models_dir: str = "data/ai_models_v4",
        raw_data_path: str = "data/raw",
        clubs_path: str = "data/clubs",
        test_size: float = 0.2,
        random_state: int = 42,
        use_smote: bool = True,
        balance_method: str = 'hybrid',
        use_class_weights: bool = True,
        n_iter_search: int = 80,
        cv_folds: int = 5,
        use_voting: bool = True,
        verbose: bool = True
    ):
        self.models_dir = Path(models_dir)
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.raw_data_path = Path(raw_data_path)
        self.clubs_path = Path(clubs_path)
        self.test_size = test_size
        self.random_state = random_state
        
        self.use_smote = use_smote and IMBLEARN_AVAILABLE
        self.balance_method = balance_method
        self.use_class_weights = use_class_weights
        self.n_iter_search = n_iter_search
        self.cv_folds = cv_folds
        self.use_voting = use_voting
        self.verbose = verbose
        
        if not FEATURE_ENGINEER_AVAILABLE:
            raise ImportError("‚ùå EnhancedFeatureEngineer bulunamadƒ±!")
        
        self.feature_engineer = EnhancedFeatureEngineer(model_path=str(self.models_dir))
        
        self.history_processor = None
        if HISTORICAL_PROCESSOR_AVAILABLE:
            self.history_processor = HistoricalDataProcessor(
                str(self.raw_data_path),
                str(self.clubs_path)
            )
        
        self.models = {
            'xgboost': None,
            'gradient_boost': None,
            'random_forest': None,
            'lightgbm': None
        }
        self.stacking_model = None
        self.voting_model = None
        self.score_model = None
        self.scaler = StandardScaler()
        
        self.class_weights = None
        
        self.metadata = {
            "version": "v4.0_optimized",
            "trained_at": datetime.now().isoformat(),
            "optimization": {
                "use_smote": self.use_smote,
                "balance_method": self.balance_method,
                "use_class_weights": self.use_class_weights,
                "n_iter_search": self.n_iter_search,
                "cv_folds": self.cv_folds,
                "use_voting": self.use_voting
            }
        }
        
        self.logger = logger
    
    def load_and_prepare_data(self) -> pd.DataFrame:
        self.logger.info("üìÇ Veri y√ºkleniyor...")
        
        all_matches = []
        
        if self.history_processor and self.raw_data_path.exists():
            for country_dir in self.raw_data_path.iterdir():
                if country_dir.is_dir():
                    try:
                        matches = self.history_processor.load_country_data(country_dir.name)
                        all_matches.extend(matches)
                        self.logger.info(f"‚úÖ {country_dir.name}: {len(matches)} ma√ß")
                    except Exception as e:
                        self.logger.warning(f"‚ö†Ô∏è {country_dir.name}: {e}")
        else:
            csv_files = list(self.raw_data_path.glob("**/*.csv"))
            for csv_file in csv_files:
                try:
                    df_temp = pd.read_csv(csv_file)
                    required_cols = ['home_team', 'away_team', 'home_score', 'away_score']
                    if all(col in df_temp.columns for col in required_cols):
                        league = csv_file.parent.name if csv_file.parent.name != 'raw' else 'Unknown'
                        df_temp['league'] = league
                        all_matches.extend(df_temp.to_dict('records'))
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è {csv_file}: {e}")
        
        if not all_matches:
            raise ValueError("‚ùå Veri bulunamadƒ±!")
        
        df = pd.DataFrame(all_matches)
        self.logger.info(f"üìä Toplam {len(df):,} ma√ß")
        return df
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        self.logger.info("üßπ Veri temizleniyor...")
        
        initial = len(df)
        
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'], errors='coerce')
            self.logger.info(f"üìÖ T√ºm sezonlar dahil: {len(df):,} ma√ß")
        
        df = df.dropna(subset=["home_team", "away_team", "home_score", "away_score"])
        
        df["home_score"] = pd.to_numeric(df["home_score"], errors='coerce').fillna(0).astype(int)
        df["away_score"] = pd.to_numeric(df["away_score"], errors='coerce').fillna(0).astype(int)
        
        df = df[(df["home_score"] >= 0) & (df["away_score"] >= 0)]
        df = df[(df["home_score"] <= 20) & (df["away_score"] <= 20)]
        
        if 'odds' in df.columns:
            def is_valid_odds(odds):
                if not isinstance(odds, dict):
                    return False
                for key in ['1', 'X', '2']:
                    val = odds.get(key, 0)
                    if not (1.1 <= val <= 50):
                        return False
                return True
            
            df = df[df['odds'].apply(is_valid_odds)]
            self.logger.info(f"üé≤ Ge√ßerli oranlar: {len(df):,} ma√ß")
        
        df["result"] = df.apply(
            lambda r: "1" if r["home_score"] > r["away_score"]
            else "2" if r["home_score"] < r["away_score"]
            else "X", axis=1
        )
        
        if 'odds' not in df.columns:
            df['odds'] = [{"1": 2.0, "X": 3.0, "2": 3.5} for _ in range(len(df))]
        
        if 'league' not in df.columns:
            df['league'] = 'Unknown'
        
        self.logger.info(f"‚úÖ {initial:,} ‚Üí {len(df):,} kayƒ±t")
        self.logger.info(f"üìä Daƒüƒ±lƒ±m: {dict(df['result'].value_counts())}")
        
        return df
    
    def extract_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        self.logger.info("üîß Feature extraction...")
        
        X_list, y_ms, y_score = [], [], []
        
        for _, row in tqdm(df.iterrows(), total=len(df), desc="Features"):
            try:
                match_data = {
                    "home_team": row["home_team"],
                    "away_team": row["away_team"],
                    "league": row.get("league", "Unknown"),
                    "odds": row.get("odds", {"1": 2.0, "X": 3.0, "2": 3.5}),
                    "date": str(row.get("date", ""))
                }
                
                features = self.feature_engineer.extract_features(match_data)
                
                if features is not None:
                    X_list.append(features)
                    y_ms.append({"1": 0, "X": 1, "2": 2}[row["result"]])
                    y_score.append(f"{row['home_score']}-{row['away_score']}")
            except:
                continue
        
        X = np.array(X_list, dtype=np.float32)
        y_ms = np.array(y_ms, dtype=np.int8)
        
        self.logger.info(f"‚úÖ Shape: {X.shape}")
        return X, y_ms, y_score
    
    def analyze_class_distribution(self, y: np.ndarray, name: str = "Dataset"):
        counter = Counter(y)
        total = len(y)
        
        self.logger.info(f"üìä {name}:")
        for cls in sorted(counter.keys()):
            count = counter[cls]
            pct = count / total * 100
            class_name = {0: "Ev (1)", 1: "Berab (X)", 2: "Depl (2)"}[cls]
            self.logger.info(f"   {class_name}: {count:6,} ({pct:5.2f}%)")
        
        imbalance = max(counter.values()) / min(counter.values())
        self.logger.info(f"   Dengesizlik: {imbalance:.2f}:1")
        return counter, imbalance
    
    def compute_class_weights(self, y_train: np.ndarray):
        if not self.use_class_weights:
            return None
        
        classes = np.unique(y_train)
        weights = compute_class_weight('balanced', classes=classes, y=y_train)
        weight_dict = {int(cls): float(w) for cls, w in zip(classes, weights)}
        
        self.logger.info(f"‚öñÔ∏è  Class weights: {weight_dict}")
        self.class_weights = weight_dict
        return weight_dict
    
    def balance_dataset(self, X_train: np.ndarray, y_train: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        if not self.use_smote:
            return X_train, y_train
        
        self.logger.info("=" * 80)
        self.logger.info(f"üîß BALANCING: {self.balance_method.upper()}")
        self.logger.info("=" * 80)
        
        counter_before, _ = self.analyze_class_distribution(y_train, "√ñNCESƒ∞")
        
        try:
            min_samples = min(counter_before.values())
            k_neighbors = min(5, min_samples - 1)
            
            if self.balance_method == 'smote':
                sampler = SMOTE(random_state=self.random_state, k_neighbors=k_neighbors)
                X_balanced, y_balanced = sampler.fit_resample(X_train, y_train)
                
            elif self.balance_method == 'smote_tomek':
                sampler = SMOTETomek(
                    smote=SMOTE(random_state=self.random_state, k_neighbors=k_neighbors),
                    random_state=self.random_state
                )
                X_balanced, y_balanced = sampler.fit_resample(X_train, y_train)
                
            elif self.balance_method == 'hybrid':
                self.logger.info("üîÑ Step 1: RandomUnderSampler...")
                rus = RandomUnderSampler(random_state=self.random_state, sampling_strategy='auto')
                X_temp, y_temp = rus.fit_resample(X_train, y_train)
                self.analyze_class_distribution(y_temp, "After UnderSampling")
                
                self.logger.info("üîÑ Step 2: SMOTE...")
                smote = SMOTE(random_state=self.random_state, k_neighbors=min(5, min(Counter(y_temp).values()) - 1))
                X_balanced, y_balanced = smote.fit_resample(X_temp, y_temp)
                
            else:
                sampler = SMOTE(random_state=self.random_state)
                X_balanced, y_balanced = sampler.fit_resample(X_train, y_train)
            
            self.analyze_class_distribution(y_balanced, "SONRASI")
            self.logger.info("=" * 80)
            
            return X_balanced, y_balanced
            
        except Exception as e:
            self.logger.error(f"‚ùå Balancing hatasƒ±: {e}")
            return X_train, y_train
    
    def train_xgboost(self, X_train, y_train, X_val, y_val):
        self.logger.info("üîß XGBoost...")
        
        base = xgb.XGBClassifier(
            objective="multi:softprob",
            num_class=3,
            random_state=self.random_state,
            n_jobs=-1,
            tree_method="hist",
            learning_rate=0.08,
            n_estimators=300,
            max_depth=6,
            subsample=0.8,
            colsample_bytree=0.8,
            gamma=0.2,
            min_child_weight=3,
            reg_lambda=1.0,
            reg_alpha=0.2,
            use_label_encoder=False,
            verbosity=0
        )

        param_dist = {
            "max_depth": [4, 5, 6],
            "learning_rate": [0.05, 0.08, 0.1],
            "subsample": [0.7, 0.8, 0.9],
            "colsample_bytree": [0.7, 0.8, 0.9],
            "n_estimators": [150, 200, 250, 300],
            "gamma": [0, 0.1, 0.2, 0.3],
            "min_child_weight": [1, 2, 3]
        }

        cv = StratifiedKFold(
            n_splits=self.cv_folds,
            shuffle=True,
            random_state=self.random_state
        )

        search = RandomizedSearchCV(
            base,
            param_dist,
            n_iter=min(self.n_iter_search, 12),
            cv=cv,
            scoring="f1_macro",
            n_jobs=-1,
            random_state=self.random_state,
            verbose=1 if self.verbose else 0
        )

        search.fit(X_train, y_train)
        model = search.best_estimator_
        
        preds = model.predict(X_val)
        acc = accuracy_score(y_val, preds)
        bal_acc = balanced_accuracy_score(y_val, preds)
        f1 = f1_score(y_val, preds, average="macro")

        self.logger.info(f"‚úÖ XGBoost: Acc={acc:.4f}, BalAcc={bal_acc:.4f}, F1={f1:.4f}")
        self.logger.info(f"   Best params: {search.best_params_}")

        return model, f1
    
    def train_gradient_boost(self, X_train, y_train, X_val, y_val):
        self.logger.info("üîß GradientBoost...")
        
        param_dist = {
            'n_estimators': [200, 300, 500, 700],
            'max_depth': [3, 5, 7, 9],
            'learning_rate': [0.05, 0.1, 0.15, 0.2],
            'subsample': [0.8, 0.9, 1.0],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4]
        }
        
        base = GradientBoostingClassifier(random_state=self.random_state)
        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)
        search = RandomizedSearchCV(
            base, param_dist,
            n_iter=self.n_iter_search,
            cv=cv,
            scoring='f1_macro',
            n_jobs=-1,
            verbose=0
        )
        
        search.fit(X_train, y_train)
        model = search.best_estimator_
        
        pred = model.predict(X_val)
        f1 = f1_score(y_val, pred, average='macro')
        
        self.logger.info(f"‚úÖ GradientBoost: F1={f1:.4f}")
        return model, f1
    
    def train_random_forest(self, X_train, y_train, X_val, y_val):
        self.logger.info("üîß RandomForest...")
        
        param_dist = {
            'n_estimators': [200, 300, 500, 700],
            'max_depth': [15, 20, 25, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None],
            'class_weight': ['balanced', 'balanced_subsample', None]
        }
        
        base = RandomForestClassifier(random_state=self.random_state, n_jobs=-1)
        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)
        search = RandomizedSearchCV(
            base, param_dist,
            n_iter=self.n_iter_search,
            cv=cv,
            scoring='f1_macro',
            n_jobs=-1,
            verbose=0
        )
        
        search.fit(X_train, y_train)
        model = search.best_estimator_
        
        pred = model.predict(X_val)
        f1 = f1_score(y_val, pred, average='macro')
        
        self.logger.info(f"‚úÖ RandomForest: F1={f1:.4f}")
        return model, f1
    
    def train_lightgbm(self, X_train, y_train, X_val, y_val):
        if not LIGHTGBM_AVAILABLE:
            return None, 0
        
        self.logger.info("üîß LightGBM...")
        
        param_dist = {
            'n_estimators': [200, 300, 500, 700],
            'max_depth': [5, 7, 9, 11],
            'learning_rate': [0.05, 0.1, 0.15],
            'num_leaves': [31, 50, 70, 100],
            'min_child_samples': [20, 30, 50],
            'subsample': [0.8, 0.9, 1.0],
            'colsample_bytree': [0.8, 0.9, 1.0],
            'class_weight': ['balanced', None] if self.use_class_weights else [None]
        }
        
        base = lgb.LGBMClassifier(
            random_state=self.random_state, 
            n_jobs=-1, 
            verbose=-1,
            force_col_wise=True
        )
        cv = StratifiedKFold(n_splits=self.cv_folds, shuffle=True, random_state=self.random_state)
        search = RandomizedSearchCV(
            base, param_dist,
            n_iter=self.n_iter_search,
            cv=cv,
            scoring='f1_macro',
            n_jobs=-1,
            verbose=0
        )
        
        search.fit(X_train, y_train)
        model = search.best_estimator_
        
        pred = model.predict(X_val)
        f1 = f1_score(y_val, pred, average='macro')
        
        self.logger.info(f"‚úÖ LightGBM: F1={f1:.4f}")
        return model, f1
    
    def train_base_models(self, X_train, X_test, y_train, y_test):
        self.logger.info("=" * 80)
        self.logger.info("üéØ BASE MODEL Eƒûƒ∞Tƒ∞Mƒ∞")
        self.logger.info("=" * 80)
        
        self.compute_class_weights(y_train)
        
        self.analyze_class_distribution(y_train, "Original")
        X_train_bal, y_train_bal = self.balance_dataset(X_train, y_train)
        
        f1_scores = {}
        
        try:
            model, f1 = self.train_xgboost(X_train_bal, y_train_bal, X_test, y_test)
            self.models['xgboost'] = model
            f1_scores['xgboost'] = f1
        except Exception as e:
            self.logger.error(f"‚ùå XGBoost: {e}")
        
        try:
            model, f1 = self.train_gradient_boost(X_train_bal, y_train_bal, X_test, y_test)
            self.models['gradient_boost'] = model
            f1_scores['gradient_boost'] = f1
        except Exception as e:
            self.logger.error(f"‚ùå GradientBoost: {e}")
        
        try:
            model, f1 = self.train_random_forest(X_train_bal, y_train_bal, X_test, y_test)
            self.models['random_forest'] = model
            f1_scores['random_forest'] = f1
        except Exception as e:
            self.logger.error(f"‚ùå RandomForest: {e}")
        
        if LIGHTGBM_AVAILABLE:
            try:
                model, f1 = self.train_lightgbm(X_train_bal, y_train_bal, X_test, y_test)
                if model:
                    self.models['lightgbm'] = model
                    f1_scores['lightgbm'] = f1
            except Exception as e:
                self.logger.error(f"‚ùå LightGBM: {e}")
        
        self.logger.info("=" * 80)
        self.logger.info("üìä SONU√áLAR (F1-Macro):")
        for name, score in sorted(f1_scores.items(), key=lambda x: x[1], reverse=True):
            self.logger.info(f"  {name}: {score:.4f}")
        
        return f1_scores
    
    def train_voting(self, X_train, X_test, y_train, y_test):
        if not self.use_voting or len([m for m in self.models.values() if m]) < 2:
            return 0.0
        
        self.logger.info("üéØ Voting Ensemble...")
        
        estimators = [(name, model) for name, model in self.models.items() if model]
        weights = [0.35, 0.30, 0.25, 0.10][:len(estimators)]
        
        voting = VotingClassifier(
            estimators=estimators,
            voting='soft',
            weights=weights,
            n_jobs=-1
        )
        
        voting.fit(X_train, y_train)
        
        pred = voting.predict(X_test)
        acc = accuracy_score(y_test, pred)
        f1 = f1_score(y_test, pred, average='macro')
        
        self.voting_model = voting
        self.logger.info(f"‚úÖ Voting: Acc={acc:.4f}, F1={f1:.4f}")
        return f1
    
    def train_stacking(self, X_train, X_test, y_train, y_test):
        if len([m for m in self.models.values() if m]) < 2:
            return 0.0
        
        self.logger.info("üéØ Stacking Ensemble...")
        
        estimators = [(name, model) for name, model in self.models.items() if model]
        final = xgb.XGBClassifier(
            max_depth=6, 
            learning_rate=0.05,
            n_estimators=100,
            random_state=self.random_state
        )
        
        stacking = StackingClassifier(
            estimators=estimators, 
            final_estimator=final, 
            cv=5, 
            n_jobs=-1,
            passthrough=True
        )
        stacking.fit(X_train, y_train)
        
        pred = stacking.predict(X_test)
        acc = accuracy_score(y_test, pred)
        f1 = f1_score(y_test, pred, average='macro')
        
        self.stacking_model = stacking
        self.logger.info(f"‚úÖ Stacking: Acc={acc:.4f}, F1={f1:.4f}")
        return f1
    
    def analyze_results(self, X_test, y_test):
        self.logger.info("\n" + "=" * 80)
        self.logger.info("üìä DETAYLI ANALƒ∞Z")
        self.logger.info("=" * 80)
        
        for name, model in self.models.items():
            if not model:
                continue
            
            pred = model.predict(X_test)
            
            self.logger.info(f"\n{name.upper()}:")
            self.logger.info(f"  Accuracy: {accuracy_score(y_test, pred):.4f}")
            self.logger.info(f"  Balanced Acc: {balanced_accuracy_score(y_test, pred):.4f}")
            self.logger.info(f"  F1-Macro: {f1_score(y_test, pred, average='macro'):.4f}")
            
            cm = confusion_matrix(y_test, pred)
            self.logger.info(f"\n  Confusion Matrix:")
            self.logger.info(f"  {cm[0]}")
            self.logger.info(f"  {cm[1]}")
            self.logger.info(f"  {cm[2]}")
        
        if self.voting_model:
            pred = self.voting_model.predict(X_test)
            self.logger.info(f"\nVOTING ENSEMBLE:")
            self.logger.info(f"  Accuracy: {accuracy_score(y_test, pred):.4f}")
            self.logger.info(f"  F1-Macro: {f1_score(y_test, pred, average='macro'):.4f}")
        
        if self.stacking_model:
            pred = self.stacking_model.predict(X_test)
            self.logger.info(f"\nSTACKING ENSEMBLE:")
            self.logger.info(f"  Accuracy: {accuracy_score(y_test, pred):.4f}")
            self.logger.info(f"  F1-Macro: {f1_score(y_test, pred, average='macro'):.4f}")
    
    def save_models(self):
        self.logger.info("üíæ Kaydediliyor...")
        
        joblib.dump({
            'models': self.models,
            'scaler': self.scaler,
            'class_weights': self.class_weights,
            'is_trained': True,
            'metadata': self.metadata
        }, self.models_dir / "ensemble_models.pkl")
        
        if self.stacking_model:
            joblib.dump({
                'model': self.stacking_model,
                'type': 'stacking'
            }, self.models_dir / "stacking_ensemble.pkl")
        
        if self.voting_model:
            joblib.dump({
                'model': self.voting_model,
                'type': 'voting'
            }, self.models_dir / "voting_ensemble.pkl")
        
        with open(self.models_dir / "feature_config.json", 'w') as f:
            json.dump(self.metadata, f, indent=2)
        
        self.logger.info(f"‚úÖ Kaydedildi: {self.models_dir}")
    
    def run_training(self):
        start = time.time()
        
        self.logger.info("=" * 80)
        self.logger.info("üöÄ OPTIMIZED ML TRAINER v4.0 - TARGET: 70%+")
        self.logger.info("=" * 80)
        self.logger.info(f"‚öôÔ∏è  Config:")
        self.logger.info(f"   - Balance Method: {self.balance_method}")
        self.logger.info(f"   - Class Weights: {self.use_class_weights}")
        self.logger.info(f"   - N Iterations: {self.n_iter_search}")
        self.logger.info(f"   - CV Folds: {self.cv_folds}")
        self.logger.info(f"   - Voting Ensemble: {self.use_voting}")
        self.logger.info("=" * 80)
        
        try:
            df = self.load_and_prepare_data()
            df = self.clean_data(df)
            
            X, y_ms, y_score = self.extract_features(df)
            self.metadata['expected_features'] = X.shape[1]
            self.metadata['total_samples'] = len(X)
            
            X_train, X_test, y_train, y_test, _, _ = train_test_split(
                X, y_ms, y_score,
                test_size=self.test_size,
                random_state=self.random_state,
                stratify=y_ms
            )
            
            self.logger.info("üîß Scaling...")
            X_train = self.scaler.fit_transform(X_train)
            X_test = self.scaler.transform(X_test)
            
            base_scores = self.train_base_models(X_train, X_test, y_train, y_test)
            self.analyze_results(X_test, y_test)
            
            voting_score = self.train_voting(X_train, X_test, y_train, y_test)
            stacking_score = self.train_stacking(X_train, X_test, y_train, y_test)
            
            self.save_models()
            
            elapsed = time.time() - start
            
            all_scores = list(base_scores.values())
            if voting_score > 0:
                all_scores.append(voting_score)
            if stacking_score > 0:
                all_scores.append(stacking_score)
            
            best_score = max(all_scores) if all_scores else 0.45
            
            best_model_name = max(base_scores.items(), key=lambda x: x[1])[0] if base_scores else None
            if best_model_name and self.models[best_model_name]:
                pred = self.models[best_model_name].predict(X_test)
                best_accuracy = accuracy_score(y_test, pred)
            else:
                best_accuracy = best_score * 0.95
            
            self.logger.info("\n" + "=" * 80)
            self.logger.info("üéâ Eƒûƒ∞Tƒ∞M TAMAMLANDI")
            self.logger.info("=" * 80)
            self.logger.info(f"‚è±Ô∏è  S√ºre: {elapsed/60:.1f} dakika")
            self.logger.info(f"üéØ En ƒ∞yi F1-Macro: {best_score:.4f}")
            self.logger.info(f"üéØ Test Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)")
            
            if voting_score > 0:
                self.logger.info(f"üéØ Voting F1: {voting_score:.4f}")
            if stacking_score > 0:
                self.logger.info(f"üéØ Stacking F1: {stacking_score:.4f}")
            
            expected_improvement = best_accuracy * 100
            if expected_improvement >= 70:
                self.logger.info(f"‚úÖ HEDEF ULA≈ûILDI! ({expected_improvement:.2f}% >= 70%)")
            elif expected_improvement >= 65:
                self.logger.info(f"‚ö†Ô∏è  Hedefe yakƒ±n! ({expected_improvement:.2f}%)")
                self.logger.info(f"üí° √ñneri: n_iter_search=100 dene veya daha fazla veri ekle")
            else:
                self.logger.info(f"‚ö†Ô∏è  Daha fazla optimizasyon gerekli ({expected_improvement:.2f}%)")
            
            return {
                "success": True,
                "best_f1_score": best_score,
                "test_accuracy": best_accuracy,
                "voting_f1": voting_score,
                "stacking_f1": stacking_score,
                "time_minutes": elapsed/60,
                "total_samples": len(X)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Hata: {e}", exc_info=True)
            return {"success": False, "error": str(e)}


class ProductionModelTrainer(ImprovedModelTrainer):
    """Alias class for backward compatibility with main.py"""
    def run_full_pipeline(self):
        """Main.py tarafƒ±ndan √ßaƒürƒ±lƒ±r (run_training wrapper'ƒ±)"""
        return self.run_training()


def main():
    print("=" * 80)
    print("üöÄ OPTIMIZED ML TRAINER v4.0 - TARGET: 70%+ ACCURACY")
    print("=" * 80)
    
    if not IMBLEARN_AVAILABLE:
        print("\n‚ùå imbalanced-learn y√ºkl√º deƒüil!")
        print("Y√ºklemek i√ßin: pip install imbalanced-learn")
        return
    
    if not FEATURE_ENGINEER_AVAILABLE:
        print("\n‚ùå enhanced_feature_engineer.py bulunamadƒ±!")
        return
    
    print("‚úÖ Baƒüƒ±mlƒ±lƒ±klar OK\n")
    
    print("‚öôÔ∏è  OPTIMAL AYARLAR:")
    print("   - balance_method: 'hybrid'")
    print("   - use_class_weights: True")
    print("   - n_iter_search: 80")
    print("   - cv_folds: 5")
    print("   - scoring: 'f1_macro'")
    print("   - use_voting: True")
    print("=" * 80)
    print()
    
    trainer = ImprovedModelTrainer(
        models_dir="data/ai_models_v4",
        use_smote=True,
        balance_method='hybrid',
        use_class_weights=True,
        n_iter_search=80,
        cv_folds=5,
        use_voting=True
    )
    
    result = trainer.run_training()
    
    if result["success"]:
        print(f"\n‚úÖ BA≈ûARILI!")
        print(f"üéØ Test Accuracy: {result['test_accuracy']:.4f} ({result['test_accuracy']*100:.2f}%)")
        print(f"üéØ En ƒ∞yi F1: {result['best_f1_score']:.4f}")
        
        if result['test_accuracy'] >= 0.70:
            print("\nüéâüéâüéâ %70 HEDEFƒ∞NE ULA≈ûILDI! üéâüéâüéâ")
        elif result['test_accuracy'] >= 0.65:
            print("\nüí™ Hedefe √ßok yakƒ±n! Biraz daha optimizasyon gerekli.")
        
        print(f"\nüìä ƒ∞statistikler:")
        print(f"   - Toplam √∂rnek: {result['total_samples']:,}")
        print(f"   - S√ºre: {result['time_minutes']:.1f} dk")
    else:
        print(f"\n‚ùå HATA: {result['error']}")


if __name__ == "__main__":
    main()
